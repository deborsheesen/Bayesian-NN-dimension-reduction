{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Source: http://www.robots.ox.ac.uk/~bradley/blog/2018/04/HMC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.variable import Variable\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def VariableCast(value, grad = False):\n",
    "    '''\n",
    "    Casts an input to torch Variable object, with gradients either False or True. \n",
    "    input\n",
    "    -----\n",
    "    value - Type: scalar, Variable object, torch.Tensor, numpy ndarray\n",
    "    grad  - Type: bool . If true then we require the gradient of that object\n",
    "    output\n",
    "    ------\n",
    "    torch.autograd.variable.Variable object\n",
    "    '''\n",
    "    if value is None:\n",
    "        return None\n",
    "    elif isinstance(value, Variable):\n",
    "        return value\n",
    "    elif torch.is_tensor(value):\n",
    "        return Variable(value, requires_grad = grad)\n",
    "    elif isinstance(value, np.ndarray):\n",
    "        tensor = torch.from_numpy(value).float()\n",
    "        return Variable(tensor, requires_grad = grad)\n",
    "    elif isinstance(value,list):\n",
    "        return Variable(torch.FloatTensor(value), requires_grad=grad)\n",
    "    else:\n",
    "        return Variable(torch.FloatTensor([value]), requires_grad = grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Integrator():\n",
    "\n",
    "    def __init__(self, potential, min_step, max_step, max_traj, min_traj):\n",
    "        self.potential = potential\n",
    "        if min_step is None:\n",
    "            self.min_step = np.random.uniform(0.01, 0.07)\n",
    "        else:\n",
    "            self.min_step = min_step\n",
    "        if max_step is None:\n",
    "            self.max_step = np.random.uniform(0.07, 0.18)\n",
    "        else:\n",
    "            self.max_step = max_step\n",
    "        if max_traj is None:\n",
    "            self.max_traj = np.random.uniform(18, 25)\n",
    "        else:\n",
    "            self.max_traj = max_traj\n",
    "        if min_traj is None:\n",
    "            self.min_traj = np.random.uniform(1, 18)\n",
    "        else:\n",
    "            self.min_traj = min_traj\n",
    "\n",
    "    def generate_new_step_traj(self):\n",
    "        ''' Generates a new step adn trajectory size  '''\n",
    "        step_size = np.random.uniform(self.min_step, self.max_step)\n",
    "        traj_size = int(np.random.uniform(self.min_traj, self.max_traj))\n",
    "        return step_size, traj_size\n",
    "\n",
    "\n",
    "    def leapfrog(self, p_init, x, grad_init):\n",
    "        '''Performs the leapfrog steps of the HMC for the specified trajectory\n",
    "        length, given by num_steps\n",
    "        Parameters\n",
    "        ----------\n",
    "            values_init\n",
    "            p_init\n",
    "            grad_init     - Description: contains the initial gradients of the joint w.r.t parameters.\n",
    "\n",
    "        Outputs\n",
    "        -------\n",
    "            x -    Description: proposed new x\n",
    "            p      -    Description: proposed new auxillary momentum\n",
    "        '''\n",
    "        step_size, traj_size = self.generate_new_step_traj()\n",
    "        values_init = x\n",
    "        self.kinetic = Kinetic(p_init)\n",
    "        # Start by updating the momentum a half-step and x by a full step\n",
    "        p = p_init + 0.5 * step_size * grad_init\n",
    "        x = values_init + step_size * self.kinetic.gauss_ke(p, grad=True)\n",
    "        for i in range(traj_size - 1):\n",
    "            # range equiv to [2:nsteps] as we have already performed the first step\n",
    "            # update momentum\n",
    "            p = p + step_size * self.potential.eval(x, grad=True)\n",
    "            # update x\n",
    "            x = x + step_size * self.kinetic.gauss_ke(p, grad=True)\n",
    "\n",
    "        # Do a final update of the momentum for a half step\n",
    "        p = p + 0.5 * step_size * self.potential.eval(x, grad=True)\n",
    "        # print('Debug p, vlaues leapfrog', p, x)\n",
    "\n",
    "        # return new proposal state\n",
    "        return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kinetic():\n",
    "    ''' A basic class that implements kinetic energies and computes gradients\n",
    "    Methods\n",
    "    -------\n",
    "    gauss_ke          : Returns KE gauss\n",
    "    laplace_ke        : Returns KE laplace\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    p    - Type       : torch.Tensor, torch.autograd.Variable,nparray\n",
    "        Size       : [1, ... , N]\n",
    "        Description: Vector of current momentum\n",
    "\n",
    "    M    - Type       : torch.Tensor, torch.autograd.Variable, nparray\n",
    "        Size       : \\mathbb{R}^{N \\times N}\n",
    "        Description: The mass matrix, defaults to identity.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, p, M = None):\n",
    "\n",
    "        if M is not None:\n",
    "            if isinstance(M, Variable):\n",
    "                self.M  = VariableCast(torch.inverse(M.data))\n",
    "            else:\n",
    "                self.M  = VariableCast(torch.inverse(M))\n",
    "        else:\n",
    "            self.M  = VariableCast(torch.eye(p.size()[0])) # inverse of identity is identity\n",
    "\n",
    "\n",
    "    def gauss_ke(self,p, grad = False):\n",
    "        '''' (p dot p) / 2 and Mass matrix M = \\mathbb{I}_{dim,dim}'''\n",
    "        self.p = VariableCast(p)\n",
    "        P = Variable(self.p.data, requires_grad=True)\n",
    "        K = 0.5 * P.t().mm(self.M).mm(P)\n",
    "\n",
    "        if grad:\n",
    "            return self.ke_gradients(P, K)\n",
    "        else:\n",
    "            return K\n",
    "    def laplace_ke(self, p, grad = False):\n",
    "        self.p = VariableCast(p)\n",
    "        P = Variable(self.p.data, requires_grad=True)\n",
    "        K = torch.sign(P).mm(self.M)\n",
    "        if grad:\n",
    "            return self.ke_gradients(P, K)\n",
    "        else:\n",
    "            return K\n",
    "    def ke_gradients(self, P, K):\n",
    "        return torch.autograd.grad([K], [P])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _grad_logp(self, logp, values):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the log pdf, with respect for\n",
    "    each parameter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    logjoint       - \n",
    "    values         - All parameters for which you need gradients.\n",
    "\n",
    "    :return: torch.autograd.Variable\n",
    "    \"\"\"\n",
    "\n",
    "    gradient_of_param = torch.autograd.grad(outputs=logp, inputs=values, retain_graph=True)[0]\n",
    "    return gradient_of_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metropolis():\n",
    "\n",
    "    def __init__(self, potential, integrator, M):\n",
    "        self.potential  = potential\n",
    "        self.integrator = integrator\n",
    "        self.M          = M\n",
    "        self.count      = 0\n",
    "\n",
    "    def sample_momentum(self,values):\n",
    "        assert(isinstance(values, Variable))\n",
    "        return Variable(torch.randn(values.data.size()))\n",
    "\n",
    "    def hamiltonian(self, logjoint, p):\n",
    "        \"\"\"Computes the Hamiltonian  given the current postion and momentum\n",
    "        H = U(x) + K(p)\n",
    "        U is the potential energy and is = -log_posterior(x)\n",
    "        Parameters\n",
    "        ----------\n",
    "        logjoint    - Type:torch.autograd.Variable\n",
    "                    Size: \\mathbb{R}^{1 \\times D}\n",
    "        p           - Type: torch.Tensor.Variable\n",
    "                    Size: \\mathbb{R}^{1 \\times D}.\n",
    "                    Description: Auxiliary momentum\n",
    "        log_potential :Function from state to position to 'energy'= -log_posterior\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        hamitonian : float\n",
    "        \"\"\"\n",
    "        T = self.kinetic.gauss_ke(p, grad=False)\n",
    "        return -logjoint + T\n",
    "    def acceptance(self, values_init, logjoint_init, grad_init):\n",
    "        '''Returns the new accepted state\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        returns accepted or rejected proposal\n",
    "        '''\n",
    "\n",
    "        # generate initial momentum\n",
    "\n",
    "        #### FLAG\n",
    "        p_init = self.sample_momentum(values_init)\n",
    "        # generate kinetic energy object.\n",
    "        self.kinetic = Kinetic(p_init, self.M)\n",
    "        # calc hamiltonian  on initial state\n",
    "        orig = self.hamiltonian(logjoint_init, p_init)\n",
    "\n",
    "        # generate proposals\n",
    "        values, p = self.integrator.leapfrog(p_init, values_init, grad_init)\n",
    "\n",
    "        # calculate new hamiltonian given current\n",
    "        logjoint_prop, _ = self.potential.eval(values, grad=False)\n",
    "\n",
    "        current = self.hamiltonian(logjoint_prop, p)\n",
    "        alpha = torch.min(torch.exp(orig - current))\n",
    "        # calculate acceptance probability\n",
    "        if isinstance(alpha, Variable):\n",
    "            p_accept = torch.min(torch.ones(1, 1), alpha.data)\n",
    "        else:\n",
    "            p_accept = torch.min(torch.ones(1, 1), alpha)\n",
    "        # print(p_accept)\n",
    "        if p_accept[0][0] > torch.Tensor(1, 1).uniform_()[0][0]:  # [0][0] dirty code to get integersr\n",
    "            # Updates count globally for target acceptance rate\n",
    "            # print('Debug : Accept')\n",
    "            # print('Debug : Count ', self.count)\n",
    "            self.count = self.count + 1\n",
    "            return values, self.count\n",
    "        else:\n",
    "            # print('Debug : reject')\n",
    "            return values_init, self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMCsampler():\n",
    "    '''\n",
    "    Notes:  the params from FOPPL graph will have to all be passed to - maybe\n",
    "    Methods\n",
    "    -------\n",
    "    leapfrog_step - preforms the integrator step in HMC\n",
    "    hamiltonian   - calculates the value of hamiltonian\n",
    "    acceptance    - calculates the acceptance probability\n",
    "    run_sampler\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    '''\n",
    "    def __init__(self, program, burn_in= 100, n_samples= 20000, M= None,  min_step= None, max_step= None,\\\n",
    "                min_traj= None, max_traj= None):\n",
    "        self.burn_in    = burn_in\n",
    "        self.n_samples  = n_samples\n",
    "        self.M          = M\n",
    "        self.potential  = program()\n",
    "        self.integrator = Integrator(self.potential, min_step, max_step, \\\n",
    "                                    min_traj, max_traj)\n",
    "        # self.dim        = dim\n",
    "\n",
    "        # TO DO : Implement a adaptive step size tuning from HMC\n",
    "        # TO DO : Have a desired target acceptance ratio\n",
    "        # TO DO : Implement a adaptive trajectory size from HMC\n",
    "\n",
    "\n",
    "    def run_sampler(self):\n",
    "        ''' Runs the hmc internally for a number of samples and updates\n",
    "        our parameters of interest internally\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_samples\n",
    "        burn_in\n",
    "\n",
    "        Output\n",
    "        ----------\n",
    "        A tensor of the number of required samples\n",
    "        Acceptance rate\n",
    "        '''\n",
    "        print(' The sampler is now running')\n",
    "        # In the future dim = # of variables will not be needed as Yuan will provide\n",
    "        # that value in the program and it shall return the required dim.\n",
    "        logjoint_init, values_init, grad_init, dim = self.potential.generate()\n",
    "        metropolis   = Metropolis(self.potential, self.integrator, self.M)\n",
    "        temp,count   = metropolis.acceptance(values_init, logjoint_init, grad_init)\n",
    "        samples      = Variable(torch.zeros(self.n_samples,dim))\n",
    "        samples[0]   = temp.data.t()\n",
    "\n",
    "\n",
    "        # Then run for loop from 2:n_samples\n",
    "        for i in range(self.n_samples-1):\n",
    "            logjoint_init, grad_init = self.potential.eval(temp, grad_loop= True)\n",
    "            temp, count = metropolis.acceptance(temp, logjoint_init, grad_init)\n",
    "            samples[i + 1, :] = temp.data.t()\n",
    "            # try:\n",
    "            #     samples[i+1,:] = temp.data.t()\n",
    "            # except RuntimeError:\n",
    "            #     print(i)\n",
    "            #     break\n",
    "            # update parameters and draw new momentum\n",
    "            if i == np.floor(self.n_samples/4) or i == np.floor(self.n_samples/2) or i == np.floor(3*self.n_samples/4):\n",
    "                print(' At interation {}'.format(i))\n",
    "\n",
    "        # Basic summary statistics\n",
    "        target_acceptance =  count / (self.n_samples)\n",
    "        samples_reduced   = samples[self.burn_in:, :]\n",
    "        mean = torch.mean(samples_reduced,dim=0, keepdim= True)\n",
    "        print()\n",
    "        print('****** EMPIRICAL MEAN/COV USING HMC ******')\n",
    "        print('empirical mean : ', mean)\n",
    "        print('Average acceptance rate is: ', target_acceptance)\n",
    "\n",
    "        return samples,samples_reduced, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_HMC = Integrator(1., None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Integrator.leapfrog of <__main__.Integrator object at 0x7f79a6a64320>>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_HMC.leapfrog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, copy, timeit, numpy.random as npr, numpy as np, torch\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import plot, show, legend\n",
    "from scipy.stats import uniform, gaussian_kde\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1000\n",
    "X = 10*torch.rand(M,2)\n",
    "Y = torch.zeros((M,3))\n",
    "Y[:,0] = torch.cos(X[:,0])*torch.cos(X[:,1])\n",
    "Y[:,1] = torch.cos(X[:,0])*torch.sin(X[:,1])\n",
    "Y[:,2] = torch.sin(X[:,0])\n",
    "\n",
    "idx = Y[:,2]>0\n",
    "Y = Y[idx,:]\n",
    "X = X[idx,:]\n",
    "Y += 0.1*torch.rand(*np.shape(Y))\n",
    "Nobs = np.shape(X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim, out_dim = np.shape(X)[1], np.shape(Y)[1]\n",
    "hidden_dim = 50\n",
    "\n",
    "nn_model = nn.Sequential(nn.Linear(in_dim, hidden_dim),\n",
    "                         nn.Tanh(),\n",
    "                         nn.Linear(hidden_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior_σ2 = 1\n",
    "error_σ2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "def log_ll(X, Y, nn_model, error_σ2=1., tempe=1.) :\n",
    "    Nobs = np.shape(X)[0]\n",
    "    loss = Nobs*nn.MSELoss()(Y, nn_model(X))\n",
    "    return -temp*loss.data/(2*error_σ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_prior(X, nn_model, prior_σ2=1., temp=1.) :\n",
    "    X_part = torch.sum(X**2)/(2*prior_σ2)\n",
    "    theta_part = torch.sum(torch.cat([param.view(-1) for param in nn_model.parameters()])**2)/(2*prior_σ2)\n",
    "    return -temp*(X_part+theta_part).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_target(X, Y, nn_model, error_σ2, prior_σ2, temp) :\n",
    "    return log_ll(X, Y, nn_model, error_σ2, temp) + log_prior(X, nn_model, prior_σ2, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  62.4331, -146.8752,   11.5203,  ...,  -69.1327,   -9.6869,\n",
       "         -18.3481])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*torch.randn(10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_seq = np.linspace(1e-2,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 10**3\n",
    "X_particles = torch.randn(n_particles, *np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/auto/pkg/python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/auto/pkg/python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/auto/pkg/python-3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in less\n",
      "  \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'theta_particles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-9c1ee716a055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX_particles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_particles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresampled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtheta_particles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta_particles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresampled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'theta_particles' is not defined"
     ]
    }
   ],
   "source": [
    "for t in trange(len(temp_seq)) :\n",
    "    n_particles = np.shape(X_particles)[0]\n",
    "    # get weights:\n",
    "    weights = np.exp([log_target(X_particles[i], Y, nn_model, error_σ2, prior_σ2, \n",
    "                                temp_seq[t]-temp_seq[t-1]) \n",
    "                          for i in range(n_particles)])\n",
    "    # resample:\n",
    "    resampled = npr.choice(a=n_particles,size=n_particles,p=weights/sum(weights),replace=True)\n",
    "    X_particles = X_particles[resampled]\n",
    "    theta_particles = theta_particles[resampled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.exp([log_target(X_particles[i], Y, nn_model, error_σ2, prior_σ2, temp_seq[t]-temp_seq[t-1]) \n",
    "              for i in range(n_particles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights /= sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([574, 163, 462, 989, 947, 332, 792, 839, 889, 741, 706, 113, 326,\n",
       "       848, 101, 349, 833, 861, 479, 177, 264, 782, 948, 542, 316,  36,\n",
       "       752,  26, 206, 713, 277, 221, 667, 881, 281, 969, 415, 480, 302,\n",
       "       276, 537, 121, 626, 284, 666, 859,  59, 215, 437, 269, 805,  67,\n",
       "       805, 853, 423, 985, 951, 140, 750, 803, 524, 163, 527, 574, 266,\n",
       "        37, 797, 190, 658, 472, 911, 293, 178,  48, 847, 202, 138,  35,\n",
       "         0, 989, 819, 637, 712,  62, 133, 227, 463, 260, 473,  54, 436,\n",
       "        86, 857, 281, 403,  95, 901,  64, 171,  61, 673, 855, 305, 513,\n",
       "       303, 777, 329,  73, 503, 862, 834, 373, 355, 887, 113, 740,  19,\n",
       "       403,  51, 429, 851,  89, 189, 312, 312, 182, 349, 576, 404, 317,\n",
       "       135, 396, 309, 678, 860, 260, 487, 965,  17, 758, 477, 826, 351,\n",
       "       746, 512, 473, 382, 118, 937, 120, 996, 847,  79, 142, 985, 685,\n",
       "       717, 238, 505, 174, 676, 538,  67, 722, 201,  26, 386, 687, 810,\n",
       "       276, 420, 814, 185, 404, 537, 906, 814,  80, 149, 557, 976, 195,\n",
       "       273, 142, 285,  56,  21, 451, 764, 299, 149,  11, 698, 553, 155,\n",
       "       761,  79, 558, 733, 672, 939, 291, 201, 179, 521, 543, 474, 426,\n",
       "       305, 345, 788, 412, 795,  59, 647, 974, 778, 506, 366, 571, 363,\n",
       "       749, 923, 357, 874, 679,  65, 650, 889, 536, 649, 506, 849, 681,\n",
       "       488, 893, 261, 215, 257,  70, 457, 547, 672, 922, 734,  18, 974,\n",
       "       660, 381, 520, 344,  29, 197, 199,  12, 180, 831,   3, 357, 631,\n",
       "       810, 112,  77, 571, 881, 761, 881, 922, 395, 832, 209, 675, 842,\n",
       "       115, 434, 442, 357, 525, 528, 475, 716, 804, 327, 581, 297, 138,\n",
       "       897, 189, 123, 385,  29, 132, 948, 418, 390, 162, 724, 296,  34,\n",
       "       118, 616, 507, 492, 760, 280, 728, 524,  85, 725, 606, 985, 120,\n",
       "        82, 456, 964, 712,  47,  96, 471, 292, 656,  36, 264, 830, 153,\n",
       "       892, 300, 296, 705, 163, 749, 742, 617, 511, 824, 969, 111, 108,\n",
       "       105, 884,  91, 467, 525, 615, 778, 620,   5, 209, 884, 477, 979,\n",
       "       334, 277, 393,  32, 540, 922, 317,  89, 820, 134,  20,   8, 218,\n",
       "       580, 257, 675, 125, 827, 401, 482, 829, 765, 707, 281, 866, 228,\n",
       "        41, 594,  15, 151, 973, 784, 323, 644,  56, 233, 892, 717,  87,\n",
       "       687, 638, 741, 237, 938, 153, 351, 204, 307, 732, 654, 756, 507,\n",
       "       144, 210, 334,  62, 381, 431,  57, 749, 729, 429, 128, 534, 844,\n",
       "       863, 153, 366, 282, 494, 936, 280,  78,  48,   3, 438, 354, 240,\n",
       "       611, 525, 271,  86, 358, 609,  31,  46, 831, 795, 881, 123, 406,\n",
       "       148, 598, 863, 448, 152, 493, 311, 241, 789, 105, 880, 917, 382,\n",
       "       643, 181, 655, 142, 809, 884,  94, 805, 746, 135, 476, 156, 746,\n",
       "       737, 486, 377, 362, 288, 954, 842, 158, 191, 324,   8, 327, 861,\n",
       "       909, 734,   6, 202, 105, 603, 907, 912, 633, 325, 484, 726, 410,\n",
       "       570, 193, 382, 720, 985, 783, 604, 529, 138, 851, 270,  89, 287,\n",
       "       435, 486, 606, 737, 757, 811, 132, 649, 673, 484, 725, 388, 357,\n",
       "       165,  65, 136, 494, 309, 963, 759, 871, 922, 116, 440, 929, 782,\n",
       "       355, 804,  46,  41, 938, 641, 685, 938, 776, 975, 280, 925, 131,\n",
       "       965, 726, 140, 388, 746, 575, 304,  48, 299, 674, 654, 476, 171,\n",
       "       588, 605, 401, 943, 346, 403, 953, 233, 791, 663, 788, 933, 525,\n",
       "       844, 846, 840, 977, 757, 550, 722, 577, 196, 153, 318, 753, 852,\n",
       "        15, 113, 556, 291, 455, 659, 145, 492, 912, 940, 442,  55, 834,\n",
       "        26, 229, 963, 757, 819, 950, 520, 266, 955, 545, 908, 584, 160,\n",
       "       964, 379, 165, 317, 719, 715, 804,  57, 382, 798, 210, 319, 167,\n",
       "       194, 840, 693, 374,  12, 173, 779, 609, 554, 920, 258, 846, 110,\n",
       "       932, 496,  55, 779, 729, 515, 267, 514, 481, 455, 301,  54, 816,\n",
       "       995, 282, 556, 276, 949, 690, 330,  68, 865, 208, 610, 361, 681,\n",
       "       627, 226, 956,   3, 452, 401,  25,  48, 361, 504, 485, 710, 442,\n",
       "       724, 719, 691, 358,  67, 358, 823, 400, 524, 780, 624,  18, 916,\n",
       "       606, 235, 709, 507, 231, 593, 628, 192, 943, 131, 411, 706, 251,\n",
       "       883, 540, 274, 928, 513, 215, 470, 512, 914, 154, 798, 117, 770,\n",
       "       184, 765, 238, 840, 323, 510, 107, 401, 368, 289, 223, 920, 156,\n",
       "       289,  84,  89, 750, 818, 295, 230,  20, 742, 637, 179,  66, 745,\n",
       "       583, 138, 919, 478, 916, 513, 263,  16, 278, 685, 653, 372,   3,\n",
       "       555, 345, 301, 395, 692, 274, 626, 496, 390, 852, 962, 252, 963,\n",
       "       357, 808, 225,  30, 137, 319, 416, 493, 427, 104,  83, 914, 613,\n",
       "       243, 759, 194, 415, 537, 499, 133, 595, 778, 418, 458, 431, 393,\n",
       "       975, 895, 304, 210, 857, 304, 469, 761, 539, 841, 762, 962, 702,\n",
       "       949,  79, 801, 500, 249, 858, 457, 194, 773, 892, 939,  30, 361,\n",
       "       224, 225, 468, 569, 853,   2, 762, 288, 421, 153, 700,   3, 874,\n",
       "       866, 942, 731, 526, 366, 854, 101, 738, 845, 773, 678, 498, 197,\n",
       "       896, 305, 574, 304, 441, 446, 666,  60, 909, 699, 730, 852,  28,\n",
       "       494, 524,  93, 844, 769, 839, 202, 203,  51, 518, 485, 548, 482,\n",
       "       891, 225,   5,  47, 750, 259,  57,  56, 899, 665, 334,  25,  29,\n",
       "       458, 533, 321, 738, 994, 229, 812, 755, 116, 998, 882, 137,  25,\n",
       "       282, 875, 210, 186, 131, 739, 495, 546, 335, 123, 552, 691, 696,\n",
       "       475, 514, 617,  80, 467, 876, 950, 162, 690, 827, 534, 620, 424,\n",
       "       809, 961, 820, 243, 798, 755, 984, 172, 793,  15, 539, 629, 840,\n",
       "       813, 686, 660,  13, 400, 920, 268,  10, 249, 475, 498, 819, 177,\n",
       "       840, 411, 360,  73, 693,  82, 191, 702, 341, 379, 282, 401, 639,\n",
       "       815, 564, 215, 177, 554, 490, 915, 268, 389, 358, 678,  87, 628,\n",
       "       382, 994, 227, 797, 615, 823, 570, 600, 747, 458, 412, 841, 332,\n",
       "       452, 525, 104, 327, 496, 699,  32, 227, 948, 911, 482,  99])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

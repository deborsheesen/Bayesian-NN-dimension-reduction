{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reference for HMC: Figure 2 of https://arxiv.org/pdf/1206.1901.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, numpy.random as npr, torch.nn as nn, copy, timeit, torch\n",
    "from torch.distributions.bernoulli import Bernoulli \n",
    "from tqdm import trange\n",
    "from HMCfunctions import *\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import plot, show, legend"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Have a class \"model\" that gives log-likelihood and gradient of log-likelihood:\n",
    "\n",
    "abstract class model (must have log-likelihood and gradient)\n",
    "sub class: whatever models we are interested in "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class general HMC \n",
    "class leapfrog \n",
    "takes in model\n",
    "model has gradient and log-likelihood \n",
    "etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient w.r.t. $\\theta$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\log p(\\theta \\mid \\sigma, x_{1:n}, y_{1:n}) \n",
    "= \n",
    "- \\sum_{i=1}^n \\frac{\\left ( \\mu_\\theta(x_i) - y_i \\right ) \\, \\nabla_\\theta \\mu_\\theta(x_i) }{\\sigma^{2k}} + \\nabla_\\theta \\log p_0(\\theta) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_in = 1\n",
    "n_h1 = 5\n",
    "# n_h2 = 10\n",
    "n_out = 2\n",
    "\n",
    "nn_model = nn.Sequential(nn.Linear(n_in, n_h1),\n",
    "                         nn.Tanh(),\n",
    "                         nn.Linear(n_h1, n_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in nn_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Randomly initialise model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=5, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.apply(init_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nobs = 10_000\n",
    "x = torch.rand(nobs, n_in)\n",
    "y = np.zeros((nobs, n_out))\n",
    "y[:,0] = list(np.cos(2*np.pi*x))\n",
    "y[:,1] = list(np.sin(2*np.pi*x))\n",
    "y = torch.from_numpy(y).float()\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get dimensions of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes =  [torch.Size([5, 1]), torch.Size([5]), torch.Size([2, 5]), torch.Size([2])]\n"
     ]
    }
   ],
   "source": [
    "shapes = get_shapes(nn_model)\n",
    "print(\"Shapes = \", shapes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i in range(10) :\n",
    "    current_nn_model = copy.deepcopy(nn_model)\n",
    "    proposed_mom, current_mom, proposed_nn_model = leapfrog(nn_model, n_leapfrog, delta_leapfrog, shapes)\n",
    "    print(criterion(current_nn_model(x),y).data/criterion(proposed_nn_model(x),y).data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for param in nn_model.parameters() :\n",
    "    print(\"Param, grad:\", param.data, param.grad)\n",
    "    \n",
    "update_grads(nn_model, x, y)\n",
    "\n",
    "for param in nn_model.parameters() :\n",
    "    print(\"Param, grad:\", param.data, param.grad)\n",
    "    \n",
    "print(\"Loss:\", nn.MSELoss()(nn_model(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  HMC\n",
    "\n",
    "* First define the MCMC chain and randomly initialise it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 10_000\n",
    "chain = []\n",
    "for shape in shapes :\n",
    "    chain_shape = list(shape)\n",
    "    chain_shape.insert(0,T)\n",
    "    chain.append(torch.randn(chain_shape, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then run HMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta_leapfrog = 5e-3\n",
    "n_leapfrog = 25\n",
    "prior_sigma = 1\n",
    "error_sigma = 1\n",
    "nn_model.apply(init_normal)\n",
    "update_grads(nn_model, x, y)\n",
    "n_accept = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1002/10000 [00:44<07:38, 19.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter   1000/10000 after    44.7 sec | accept_rate 1.170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1152/10000 [00:54<09:30, 15.50it/s]"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for t in trange(T) : \n",
    "    nn_model, a = HMC_1step(nn_model, n_leapfrog, delta_leapfrog, shapes, x, y, criterion, prior_sigma)\n",
    "    n_accept += a\n",
    "    update_grads(nn_model, x, y)\n",
    "    for (i,param) in enumerate(nn_model.parameters()) :\n",
    "        chain[i][t] = param.data\n",
    "        \n",
    "    if ((t+1) % (int(T/10)) == 0) or (t+1) == T :\n",
    "        accept_rate = float(n_accept) / float(t+1)\n",
    "        print(\"iter %6d/%d after %7.1f sec | accept_rate %.3f\" % (\n",
    "            t+1, T, time() - start_time, accept_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESS's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ESS, means, Vars = find_ESS(chain, shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = 5\n",
    "plt.figure(figsize=(14,2))\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.subplot(131)\n",
    "plt.plot(ESS, 'o', markersize=ms)\n",
    "plt.title(\"ESSs\")\n",
    "plt.subplot(132)\n",
    "plt.plot(means, 'o', markersize=ms)\n",
    "plt.title(\"Posterior means\")\n",
    "plt.subplot(133)\n",
    "plt.plot(Vars, 'o', markersize=ms)\n",
    "plt.title(\"Posterior variances\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is some bug, because the posterior seems to be basically the prior. Increasing the number of data points does not seem to make any difference, only the prior variance makes a difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(nn_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1000  loss:  0.4030824899673462\n",
      "epoch:  2000  loss:  0.3654867112636566\n",
      "epoch:  3000  loss:  0.3555649220943451\n",
      "epoch:  4000  loss:  0.3529464900493622\n",
      "epoch:  5000  loss:  0.3522554636001587\n",
      "epoch:  6000  loss:  0.3520731031894684\n",
      "epoch:  7000  loss:  0.35202497243881226\n",
      "epoch:  8000  loss:  0.3520122468471527\n",
      "epoch:  9000  loss:  0.3520089089870453\n",
      "epoch:  10000  loss:  0.35200801491737366\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10_000):\n",
    "    # Forward Propagation\n",
    "    y_pred = nn_model(x)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if (epoch+1)%1_000 == 0 :\n",
    "        print('epoch: ', epoch+1,' loss: ', loss.item())\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0465],\n",
      "        [-1.9168]])\n",
      "tensor([-0.0425,  0.9472])\n"
     ]
    }
   ],
   "source": [
    "for param in nn_model.parameters() :\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
